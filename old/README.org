#+TITLE: Acksin Autotune
#+AUTHOR: Acksin
#+OPTIONS: html-postamble:nil body-only: t

#+begin_quote

#+end_quote

* Introduction

Autotune is a server tuning assistant which helps make software
perform optimally on your servers.

Acksin Autotune makes sure that your Linux servers are utilized as
efficiently as possible. It does this by tuning Linux to get the best
performance for your server software and also by looking over
application configuration to make sure that it is configured
optimally.

* Usage

** Help
#+begin_src sh
autotune --help
#+end_src

#+RESULTS:

** List Signatures

#+begin_src sh :results output code :exports both
autotune list
#+end_src

#+RESULTS:
#+BEGIN_SRC sh
{
  "Open": [
    "fast-server",
    "fs",
    "io",
    "memory",
    "networking"
  ],
  "Startup": [
    "golang",
    "nginx",
    "nodejs"
  ]
}
#+END_SRC

** Premium Signatures

To use the premium signatures the environment variable
=ACKSIN_API_KEY= needs to be set. This will be used to validate your
subscription. A new API key can be generated on the [[http://www.acksin.com/fugue/console?#/credentials/][Fugue Console
Credentials]] page.

* Tuning

** Show Signature

The following command shows the full set of changes that should be
done on the machine outputted as JSON. Useful for getting an overview
of what will be changing or piping the info through a tool like =jq=.

#+begin_src sh :results output code :exports both
autotune sig networking
#+end_src

#+RESULTS:
#+BEGIN_SRC sh
{
  "Name": "networking",
  "Subscription": 0,
  "Description": "Settings for high performance networking",
  "Documentation": "TODO: These setting are sort of set in stone but I feel that they can adapt as the system is being used. We don't have to set them to the values but we can migrate and change as we learn more about the system and tune it appropriately.",
  "References": [
    "http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html",
    "https://rtcamp.com/tutorials/linux/sysctl-conf/",
    "https://fasterdata.es.net/host-tuning/linux/",
    "http://cherokee-project.com/doc/other_os_tuning.html",
    "https://easyengine.io/tutorials/linux/sysctl-conf/",
    "https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf"
  ],
  "ProcFS": {
    "net.core.netdev_max_backlog": {
      "Value": "30000",
      "Description": "The number of incoming connections on the backlog queue. The maximum number of packets queued on the INPUT side."
    },
    "net.core.rmem_max": {
      "Value": "16777216",
      "Description": "The size of the receive buffer for all the sockets. 16MB per socket."
    },
    "net.core.somaxconn": {
      "Value": "16096",
      "Description": "The maximum number of queued sockets on a connection."
    },
    "net.core.wmem_max": {
      "Value": "16777216",
      "Description": "The size of the buffer for all the sockets. 16MB per socket."
    },
    "net.ipv4.ip_local_port_range": {
      "Value": "1024 65535",
      "Description": "On a typical machine there are around 28000 ports available to be bound to. This number can get exhausted quickly if there are many connections. We will increase this."
    },
    "net.ipv4.tcp_fin_timeout": {
      "Value": "15",
      "Description": "Usually, the Linux kernel holds a TCP connection even after it is closed for around two minutes. This means that there may be a port exhaustion as the kernel waits to close the connections. By moving the fin_timeout to 15 seconds we drastically reduce the length of time the kernel is waiting for the socket to get any remaining packets."
    },
    "net.ipv4.tcp_max_syn_backlog": {
      "Value": "20480",
      "Description": "Increase the number syn requests allowed. Sets how many half-open connections to backlog queue"
    },
    "net.ipv4.tcp_max_tw_buckets": {
      "Value": "400000",
      "Description": "Increase the tcp-time-wait buckets pool size to prevent simple DOS attacks"
    },
    "net.ipv4.tcp_no_metrics_save": {
      "Value": "1",
      "Description": "TCP saves various connection metrics in the route cache when the connection closes so that connections established in the near future can use these to set initial conditions. Usually, this increases overall performance, but may sometimes cause performance degradation."
    },
    "net.ipv4.tcp_rmem": {
      "Value": "4096 87380 16777216",
      "Description": "(min, default, max): The sizes of the receive buffer for the IP protocol."
    },
    "net.ipv4.tcp_syn_retries": {
      "Value": "2",
      "Description": "Number of times initial SYNs for a TCP connection attempt will be retransmitted for outgoing connections."
    },
    "net.ipv4.tcp_synack_retries": {
      "Value": "2",
      "Description": "This setting determines the number of SYN+ACK packets sent before the kernel gives up on the connection"
    },
    "net.ipv4.tcp_syncookies": {
      "Value": "1",
      "Description": "Security to prevent DDoS attacks. http://cr.yp.to/syncookies.html"
    },
    "net.ipv4.tcp_tw_reuse": {
      "Value": "1"
    },
    "net.ipv4.tcp_wmem": {
      "Value": "4096 65536 16777216",
      "Description": "(min, default, max): The sizes of the write buffer for the IP protocol."
    },
    "net.netfilter.nf_conntrack_max": {
      "Value": "200000",
      "Description": "The max is double the previous value. https://wiki.khnet.info/index.php/Conntrack_tuning"
    }
  },
  "SysFS": {
    "/sys/module/nf_conntrack/parameters/hashsize": {
      "Value": "50000"
    }
  },
  "Vars": {
    "nfConntrackMax": 200000
  }
}
#+END_SRC

#+RESULTS:

** ProcFS Changes

#+begin_src sh :results output code :exports both
autotune procfs fs
#+end_src

#+RESULTS:
#+BEGIN_SRC sh
vm.dirty_background_ratio=5
vm.dirty_expire_centisecs=1200
vm.dirty_ratio=80
#+END_SRC

#+RESULTS:

** SysFS Changes

#+begin_src sh :results output code :exports both
autotune sysfs io
#+end_src

#+RESULTS:
#+BEGIN_SRC sh
/sys/block/*/queue/read_ahead_kb=256
/sys/block/*/queue/rq_afinity=2
/sys/block/*/queue/scheduler=noop
#+END_SRC

#+RESULTS:

** Environment Variable Changes

 #+begin_src sh :results output code :exports both
 autotune env golang
 #+end_src

 #+RESULTS:
 #+BEGIN_SRC sh
#+END_SRC

 #+RESULTS:

#+begin_src ruby :results output drawer :exports results
  require 'json'

  sigs = JSON.parse(`./autotune list`)

  ["Open"].each do |st|
    puts "* #{st} Signatures"

    sigs[st].each do |s|
      sigInfo = JSON.parse(`./autotune sig #{s}`)

      puts "** #{sigInfo["Name"]}"
      puts
      puts sigInfo["Documentation"]
      puts

      ["ProcFS", "SysFS", "Env", "Files"].each do |type|
        if !!sigInfo[type]
          puts "*** #{type}"
          puts
          puts "#+ATTR_HTML: :class table"
          puts "|#{type} Key|Description|"
          puts "| <10> |||" if type == "Env"
          sigInfo[type].each do |k, v|
            puts "|=#{k}=|#{v["Description"].gsub("\n", ' ') rescue ""}|"
          end
        end
      end

      if !!sigInfo["Deps"] && !sigInfo["Deps"].empty?
        puts "*** Dependencies"
        puts
        sigInfo["Deps"].each do |k|
          puts " - [[#{k}][#{k}]]"
        end
        puts
      end

      if !!sigInfo["References"] && !sigInfo["References"].empty?
        puts "*** References"
        puts
        sigInfo["References"].each do |k|
          puts " - [[#{k}][#{k}]]"
        end
        puts
      end
    end
  end
#+end_src

#+RESULTS:
:RESULTS:
* Open Signatures
** fast-server



*** ProcFS

#+ATTR_HTML: :class table
|ProcFS Key|Description|
|=net.core.netdev_max_backlog=|The number of incoming connections on the backlog queue. The maximum number of packets queued on the INPUT side.|
|=net.core.rmem_max=|The size of the receive buffer for all the sockets. 16MB per socket.|
|=net.core.somaxconn=|The maximum number of queued sockets on a connection.|
|=net.core.wmem_max=|The size of the buffer for all the sockets. 16MB per socket.|
|=net.ipv4.ip_local_port_range=|On a typical machine there are around 28000 ports available to be bound to. This number can get exhausted quickly if there are many connections. We will increase this.|
|=net.ipv4.tcp_fin_timeout=|Usually, the Linux kernel holds a TCP connection even after it is closed for around two minutes. This means that there may be a port exhaustion as the kernel waits to close the connections. By moving the fin_timeout to 15 seconds we drastically reduce the length of time the kernel is waiting for the socket to get any remaining packets.|
|=net.ipv4.tcp_max_syn_backlog=|Increase the number syn requests allowed. Sets how many half-open connections to backlog queue|
|=net.ipv4.tcp_max_tw_buckets=|Increase the tcp-time-wait buckets pool size to prevent simple DOS attacks|
|=net.ipv4.tcp_no_metrics_save=|TCP saves various connection metrics in the route cache when the connection closes so that connections established in the near future can use these to set initial conditions. Usually, this increases overall performance, but may sometimes cause performance degradation.|
|=net.ipv4.tcp_rmem=|(min, default, max): The sizes of the receive buffer for the IP protocol.|
|=net.ipv4.tcp_syn_retries=|Number of times initial SYNs for a TCP connection attempt will be retransmitted for outgoing connections.|
|=net.ipv4.tcp_synack_retries=|This setting determines the number of SYN+ACK packets sent before the kernel gives up on the connection|
|=net.ipv4.tcp_syncookies=|Security to prevent DDoS attacks. http://cr.yp.to/syncookies.html|
|=net.ipv4.tcp_tw_reuse=||
|=net.ipv4.tcp_wmem=|(min, default, max): The sizes of the write buffer for the IP protocol.|
|=net.netfilter.nf_conntrack_max=|The max is double the previous value. https://wiki.khnet.info/index.php/Conntrack_tuning|
|=proc.min_free_kbytes=|Amount of memory to keep free. Don't want to make this too high as Linux will spend more time trying to reclaim memory.|
|=vm.dirty_background_ratio=|Contains, as a percentage of total available memory that contains free pages and reclaimable pages, the number of pages at which the background kernel flusher threads will start writing out dirty data.|
|=vm.dirty_expire_centisecs=|This tunable is used to define when dirty data is old enough to be eligible for writeout by the kernel flusher threads.  It is expressed in 100'ths of a second.  Data which has been dirty in-memory for longer than this interval will be written out next time a flusher thread wakes up. |
|=vm.dirty_ratio=|Contains, as a percentage of total available memory that contains free pages and reclaimable pages, the number of pages at which a process which is generating disk writes will itself start writing out dirty data. This value is high but should be lowered for a database application.|
|=vm.swappiness=| Disable swapping and clear the file system page cache to free memory first.|
*** SysFS

#+ATTR_HTML: :class table
|SysFS Key|Description|
|=/sys/block/*/queue/read_ahead_kb=||
|=/sys/block/*/queue/rq_afinity=||
|=/sys/block/*/queue/scheduler=||
|=/sys/kernel/mm/transparent_hugepage/enabled=|Explit huge page usage making the page size of 2 or 4 MB instead of 4kb. Should reduce CPU overhead and improve MMU page translation.|
|=/sys/module/nf_conntrack/parameters/hashsize=||
** fs



*** ProcFS

#+ATTR_HTML: :class table
|ProcFS Key|Description|
|=vm.dirty_background_ratio=|Contains, as a percentage of total available memory that contains free pages and reclaimable pages, the number of pages at which the background kernel flusher threads will start writing out dirty data.|
|=vm.dirty_expire_centisecs=|This tunable is used to define when dirty data is old enough to be eligible for writeout by the kernel flusher threads.  It is expressed in 100'ths of a second.  Data which has been dirty in-memory for longer than this interval will be written out next time a flusher thread wakes up. |
|=vm.dirty_ratio=|Contains, as a percentage of total available memory that contains free pages and reclaimable pages, the number of pages at which a process which is generating disk writes will itself start writing out dirty data. This value is high but should be lowered for a database application.|
*** Files

#+ATTR_HTML: :class table
|Files Key|Description|
|=/etc/fstab:discard=|Avoid having a discard mount attribute as every time a file is deleted the SSD will also do a TRIM for future writing. This will increase time it takes to delete a file. Better option is to run a daily/weekly cron.|
|=/etc/fstab:noattime=||
|=/etc/security/limits.conf=|Every user has unlimited file descriptors available for them upping the limit from the default 1024. This allows things like increasing the number of connections etc.|
*** References

 - [[https://tweaked.io/guide/kernel/][https://tweaked.io/guide/kernel/]]
 - [[http://blog.neutrino.es/2013/howto-properly-activate-trim-for-your-ssd-on-linux-fstrim-lvm-and-dmcrypt/][http://blog.neutrino.es/2013/howto-properly-activate-trim-for-your-ssd-on-linux-fstrim-lvm-and-dmcrypt/]]

** io



*** SysFS

#+ATTR_HTML: :class table
|SysFS Key|Description|
|=/sys/block/*/queue/read_ahead_kb=||
|=/sys/block/*/queue/rq_afinity=||
|=/sys/block/*/queue/scheduler=||
*** References

 - [[http://www.brendangregg.com/linuxperf.html][http://www.brendangregg.com/linuxperf.html]]

** memory



*** ProcFS

#+ATTR_HTML: :class table
|ProcFS Key|Description|
|=proc.min_free_kbytes=|Amount of memory to keep free. Don't want to make this too high as Linux will spend more time trying to reclaim memory.|
|=vm.swappiness=| Disable swapping and clear the file system page cache to free memory first.|
*** SysFS

#+ATTR_HTML: :class table
|SysFS Key|Description|
|=/sys/kernel/mm/transparent_hugepage/enabled=|Explit huge page usage making the page size of 2 or 4 MB instead of 4kb. Should reduce CPU overhead and improve MMU page translation.|
** networking

TODO: These setting are sort of set in stone but I feel that they can adapt as the system is being used. We don't have to set them to the values but we can migrate and change as we learn more about the system and tune it appropriately.

*** ProcFS

#+ATTR_HTML: :class table
|ProcFS Key|Description|
|=net.core.netdev_max_backlog=|The number of incoming connections on the backlog queue. The maximum number of packets queued on the INPUT side.|
|=net.core.rmem_max=|The size of the receive buffer for all the sockets. 16MB per socket.|
|=net.core.somaxconn=|The maximum number of queued sockets on a connection.|
|=net.core.wmem_max=|The size of the buffer for all the sockets. 16MB per socket.|
|=net.ipv4.ip_local_port_range=|On a typical machine there are around 28000 ports available to be bound to. This number can get exhausted quickly if there are many connections. We will increase this.|
|=net.ipv4.tcp_fin_timeout=|Usually, the Linux kernel holds a TCP connection even after it is closed for around two minutes. This means that there may be a port exhaustion as the kernel waits to close the connections. By moving the fin_timeout to 15 seconds we drastically reduce the length of time the kernel is waiting for the socket to get any remaining packets.|
|=net.ipv4.tcp_max_syn_backlog=|Increase the number syn requests allowed. Sets how many half-open connections to backlog queue|
|=net.ipv4.tcp_max_tw_buckets=|Increase the tcp-time-wait buckets pool size to prevent simple DOS attacks|
|=net.ipv4.tcp_no_metrics_save=|TCP saves various connection metrics in the route cache when the connection closes so that connections established in the near future can use these to set initial conditions. Usually, this increases overall performance, but may sometimes cause performance degradation.|
|=net.ipv4.tcp_rmem=|(min, default, max): The sizes of the receive buffer for the IP protocol.|
|=net.ipv4.tcp_syn_retries=|Number of times initial SYNs for a TCP connection attempt will be retransmitted for outgoing connections.|
|=net.ipv4.tcp_synack_retries=|This setting determines the number of SYN+ACK packets sent before the kernel gives up on the connection|
|=net.ipv4.tcp_syncookies=|Security to prevent DDoS attacks. http://cr.yp.to/syncookies.html|
|=net.ipv4.tcp_tw_reuse=||
|=net.ipv4.tcp_wmem=|(min, default, max): The sizes of the write buffer for the IP protocol.|
|=net.netfilter.nf_conntrack_max=|The max is double the previous value. https://wiki.khnet.info/index.php/Conntrack_tuning|
*** SysFS

#+ATTR_HTML: :class table
|SysFS Key|Description|
|=/sys/module/nf_conntrack/parameters/hashsize=||
*** References

 - [[http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html][http://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux.html]]
 - [[https://rtcamp.com/tutorials/linux/sysctl-conf/][https://rtcamp.com/tutorials/linux/sysctl-conf/]]
 - [[https://fasterdata.es.net/host-tuning/linux/][https://fasterdata.es.net/host-tuning/linux/]]
 - [[http://cherokee-project.com/doc/other_os_tuning.html][http://cherokee-project.com/doc/other_os_tuning.html]]
 - [[https://easyengine.io/tutorials/linux/sysctl-conf/][https://easyengine.io/tutorials/linux/sysctl-conf/]]
 - [[https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf][https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf]]

:END:
